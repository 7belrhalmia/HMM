{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Contributeurs : BELRHALMIA WALID et Mohamed Ali Darghouth et Ghiles Sidi Saïd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question 1: Dry run: Train a first-order HMM using the training data. This is basically what we did in lab sessions for POS-tagging. Compute the error rate (at the character level) and compare this results with the dummiest classifier that just do nothing. You can also compute the number of errors your model can correct and the number of errors your model creates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des packages utiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Train and Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10 =   pickle.load(open('/Users/belrhalmia/Documents/TC4/typos-data/train10.pkl', 'rb'))\n",
    "\n",
    "train20 =   pickle.load(open('/Users/belrhalmia/Documents/TC4/typos-data/train20.pkl', 'rb'))\n",
    "\n",
    "test10 =  pickle.load(open('/Users/belrhalmia/Documents/TC4/typos-data/test10.pkl', 'rb'))\n",
    "\n",
    "test20 =  pickle.load(open('/Users/belrhalmia/Documents/TC4/typos-data/test20.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les lettres et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print \"HMM creating with: \"\n",
    "            \n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    " \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "                \n",
    "            print \"States:\\n\", self.Y_index\n",
    "            print \"Observations:\\n\", self.X_index\n",
    "\n",
    "\n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "        \n",
    "        \"\"\"From one each word\n",
    "        - extract the letters and correction of rache one\n",
    "        - (letterObservation,LetterState)\n",
    "        \"\"\"\n",
    "        def data2indices(self, word): \n",
    "    \n",
    "            carracterObservation = list()\n",
    "            carracterState  = list()\n",
    "            for carracter in word:\n",
    "                observation = carracter[0]\n",
    "                state = carracter[1]\n",
    "                if observation in self.X_index:\n",
    "                    carracterObservation.append(self.X_index[observation])\n",
    "                else:\n",
    "                    carracterObservation.append(UNKid)\n",
    "                carracterState.append(self.Y_index[state])\n",
    "            return carracterObservation,carracterState\n",
    "        \n",
    "        def observation_estimation(self, cpairs):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "            # fill with counts\n",
    "            for pair in cpairs: \n",
    "                observation=pair[0]\n",
    "                state=pair[1]\n",
    "                cpt=cpairs[pair]\n",
    "                k = 0 # for <unk>\n",
    "                if observation in self.X_index: \n",
    "                    k=self.X_index[observation]\n",
    "                i=self.Y_index[state]\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def transition_estimation(self, ctrans):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in ctrans:\n",
    "                i=self.Y_index[pair[0]]\n",
    "                j=self.Y_index[pair[1]]\n",
    "                self.transition_proba[i,j]=ctrans[pair]\n",
    "            # normalize\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N)\n",
    "     \n",
    "            \n",
    "        def init_estimation(self, cinits):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for state in cinits:\n",
    "                i=self.Y_index[state]\n",
    "                self.initial_state_proba[i]=cinits[state]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "        \n",
    "        def supervised_training(self, cpairs, ctrans,cinits):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(cpairs)\n",
    "            self.transition_estimation(ctrans)\n",
    "            self.init_estimation(cinits)\n",
    "            \n",
    "        def viterbi( self, observation ):\n",
    "            \"\"\"Find the most probable state sequence\n",
    "            \"\"\"\n",
    "            product = zeros( self.N, float )\n",
    "            delta = zeros( self.N, float )\n",
    "            max_proba = zeros( (len(observation), self.N), int )\n",
    "            modified_delta = zeros( self.N, float ) # modifier le délta\n",
    "            # pour la première lettre de l'observation de chaque mot on multiplie sa matrice de probabilité d'émission(de chaque état) par la matrice de probabilité d'états initiaux.\n",
    "            delta = self.observation_proba[observation[0]] * self.initial_state_proba   \n",
    "            for L in xrange(1, len(observation)):\n",
    "                for K in range(self.N):\n",
    "                    max_proba[L, K] = multiply( delta, self.transition_proba[:, K]).argmax()       \n",
    "                    modified_delta[K] = multiply( delta, self.transition_proba[:, K])[multiply( delta, self.transition_proba[:, K]).argmax()] * self.observation_proba[observation[L], K] \n",
    "               \n",
    "                pivot=modified_delta\n",
    "                modified_delta=delta\n",
    "                delta  = pivot\n",
    "            best_path = [delta.argmax()]                        \n",
    "            for r in max_proba[-1:0:-1]:\n",
    "                best_path.append( r[best_path[-1]] )                 \n",
    "            best_path.reverse()\n",
    "            return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build different count tables to train a HMM1. Each count table is a dictionnary.\n",
    "    Returns:\n",
    "    * c_observation: No correct Letter Count (Observation Letter)\n",
    "    * c_state: Correct Letters  ( State Letters)\n",
    "    * c_pairs: count of pairs (nncorrectedletr,correctedletr)\n",
    "    * c_transitions: count bigram\n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "        \n",
    "    c_carracterObservation = dict()\n",
    "    c_carracterState = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    \n",
    "    \n",
    "    for word in train:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple=word[i]\n",
    "            observation = couple[0]\n",
    "            state = couple[1]\n",
    "            # carracter counts\n",
    "            if observation in c_carracterObservation:\n",
    "                c_carracterObservation[observation]=c_carracterObservation[observation]+1\n",
    "            else:\n",
    "                c_carracterObservation[observation]=1\n",
    "            # carracter  counts\n",
    "            if state in c_carracterState:\n",
    "                c_carracterState[state] = c_carracterState[state]+1\n",
    "            else:\n",
    "                c_carracterState[state]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans = (word[i-1][1],state)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if state in c_inits:\n",
    "                    c_inits[state]=c_inits[state]+1\n",
    "                else:\n",
    "                    c_inits[state]=1\n",
    "                    \n",
    "    return c_carracterObservation ,c_carracterState,c_pairs, c_transitions, c_inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(c_carracterObservation, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for carracter in c_carracterObservation:\n",
    "        if c_carracterObservation[carracter] >= threshold:\n",
    "            voc.append(carracter)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "27 observations\n",
      "States:\n",
      "{'a': 0, 'c': 1, 'b': 2, 'e': 3, 'd': 4, 'g': 5, 'f': 6, 'i': 7, 'h': 8, 'k': 9, 'j': 10, 'm': 11, 'l': 12, 'o': 13, 'n': 14, 'q': 15, 'p': 16, 's': 17, 'r': 18, 'u': 19, 't': 20, 'w': 21, 'v': 22, 'y': 23, 'x': 24, 'z': 25}\n",
      "Observations:\n",
      "{'a': 1, 'c': 2, 'b': 3, 'e': 4, 'd': 5, 'g': 6, 'f': 7, 'i': 8, 'h': 9, 'k': 10, 'j': 11, 'm': 12, 'l': 13, 'o': 14, 'n': 15, 'q': 16, 'p': 17, 's': 18, 'r': 19, 'u': 20, 't': 21, 'w': 22, 'v': 23, 'y': 24, 'x': 25, 'z': 26, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "ccarractero,ccarracterS,cpairs,ctrans,cinits = make_counts(train10)\n",
    "vocab = make_vocab(ccarractero,10)\n",
    "hmm = HMM(state_list=ccarracterS.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None)\n",
    "hmm.supervised_training(cpairs, ctrans, cinits)\n",
    "hmm.observation_estimation(cpairs)\n",
    "hmm.transition_estimation(ctrans)\n",
    "hmm.init_estimation(cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie Test\n",
    "Dans cette partie nous allons appliquer notre hmm sur les deux fichiers de test : test10 (qui contient 10% d'erreurs') et test20 (qui contient 20% d'erreurs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total de tous les caractéres dans le fichier de test : 7320\n",
      "Le nombre d'erreurs avant viterbi : 745\n",
      "Le nombre d'erreurs que le modèle crée : 6\n",
      "Le nombre d'erreurs que le modèle corrige : 201\n",
      "Le nombre de carractère correcte par viterbi :6777\n",
      "L'accuarcy du modèle est :92.5819672131 %\n",
      "L'erreur du modèle est : 7.41803278689 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "correct_carracter=0\n",
    "total_carracter=0\n",
    "error_before_viterbi = 0\n",
    "correction_error=0\n",
    "carracter_corrected=0\n",
    "total_carracter=0\n",
    "\n",
    "for word in test10:\n",
    "    \n",
    "    observations,states = hmm.data2indices(word)\n",
    "    \n",
    "    # meilleur séquence déterminer par l'algorithme de viterbi \n",
    "    \n",
    "    meilleur_sequence = hmm.viterbi(observations)  \n",
    "    \n",
    "    # calcul du nombre total des caractères cela va nous aider lors du calcul de l'accuracy et de l'erreur\n",
    "    \n",
    "    total_carracter= total_carracter + len(states)\n",
    "    \n",
    "    #Nombre d'erreurs avant d'avoir appliqué viterbi\n",
    "    \n",
    "    error_before_viterbi += sum (np.array([-1+x  for x in observations]) != np.array(states))\n",
    "    \n",
    "    #Nombre de caractères corrects par viterbi\n",
    "    \n",
    "    correct_carracter = correct_carracter + sum(np.array( meilleur_sequence)==np.array(states))\n",
    "\n",
    "    \n",
    "    # dans cette condition on va voir les mots pour lesquels Viterbi s'est trompé (on compare la meilleure séquence avec l'état du mot).\n",
    "    # pour les mots où viterbi s'est trompé on compare la meilleure séquence avec l'observation, et comme ça on va obtenir les caractères que viterbi à corrigé mais d'une manière fausse.\n",
    "    \n",
    "    \n",
    "    if (np.array(meilleur_sequence) != np.array(states)).all():\n",
    "        correction_error = correction_error +  sum(np.array ([-1+x  for x in observations]) != np.array(meilleur_sequence))\n",
    "        \n",
    "    \n",
    "    # nombre d'erreurs corrigées par HMM  \n",
    "    if (np.array(meilleur_sequence) == np.array(states)).all():\n",
    "        carracter_corrected += sum(np.array ([-1+x  for x in observations]) != np.array(meilleur_sequence))\n",
    "    \n",
    "print 'Le nombre total de tous les caractéres dans le fichier de test : ' +str(total_carracter)\n",
    "print \"Le nombre d'erreurs avant viterbi : \" + str(error_before_viterbi)    \n",
    "print \"Le nombre d'erreurs que le modèle crée : \" + str(correction_error)\n",
    "print \"Le nombre d'erreurs que le modèle corrige : \" + str(carracter_corrected)\n",
    "print 'Le nombre de carractère correcte par viterbi :' +str(correct_carracter)\n",
    "print \"L'accuarcy du modèle est :\"+str(correct_carracter*100.0/total_carracter ) + \" %\"\n",
    "print \"L'erreur du modèle est : \"+str(100 - correct_carracter*100.0/total_carracter ) + \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total de tous les caractéres dans le fichier de test : 16691\n",
      "Le nombre d'erreurs avant viterbi : 3239\n",
      "Le nombre d'erreurs que le modèle crée : 26\n",
      "Le nombre d'erreurs que le modèle corrige : 655\n",
      "Le nombre de carractère correcte par viterbi :14433\n",
      "L'accuarcy du modèle est :86.4717512432 %\n",
      "L'erreur du modèle est : 13.5282487568 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "correct_carracter=0\n",
    "total_carracter=0\n",
    "error_before_viterbi = 0\n",
    "correction_error=0\n",
    "carracter_corrected=0\n",
    "total_carracter=0\n",
    "\n",
    "for word in test20:\n",
    "    \n",
    "    observations,states = hmm.data2indices(word)\n",
    "    \n",
    "    # meilleur séquence déterminer par l'algorithme de viterbi \n",
    "    \n",
    "    meilleur_sequence = hmm.viterbi(observations)  \n",
    "    \n",
    "    # calcul du nombre total des caractères cela va nous aider lors du calcul de l'accuracy et de l'erreur\n",
    "    \n",
    "    total_carracter= total_carracter + len(states)\n",
    "    \n",
    "    #Nombre d'erreurs avant d'avoir appliqué viterbi\n",
    "    \n",
    "    error_before_viterbi += sum (np.array([-1+x  for x in observations]) != np.array(states))\n",
    "    \n",
    "    #Nombre de caractères corrects par viterbi\n",
    "    \n",
    "    correct_carracter = correct_carracter + sum(np.array( meilleur_sequence)==np.array(states))\n",
    "\n",
    "    \n",
    "    # dans cette condition on va voir les mots pour lesquels Viterbi s'est trompé (on compare la meilleure séquence avec l'état du mot).\n",
    "    # pour les mots où viterbi s'est trompé on compare la meilleure séquence avec l'observation, et comme ça on va obtenir les caractères que viterbi à corrigé mais d'une manière fausse.\n",
    "    \n",
    "    \n",
    "    if (np.array(meilleur_sequence) != np.array(states)).all():\n",
    "        correction_error = correction_error +  sum(np.array ([-1+x  for x in observations]) != np.array(meilleur_sequence))\n",
    "        \n",
    "    \n",
    "    # nombre d'erreurs corrigées par HMM  \n",
    "    if (np.array(meilleur_sequence) == np.array(states)).all():\n",
    "        carracter_corrected += sum(np.array ([-1+x  for x in observations]) != np.array(meilleur_sequence))\n",
    "    \n",
    "print 'Le nombre total de tous les caractéres dans le fichier de test : ' +str(total_carracter)\n",
    "print \"Le nombre d'erreurs avant viterbi : \" + str(error_before_viterbi)    \n",
    "print \"Le nombre d'erreurs que le modèle crée : \" + str(correction_error)\n",
    "print \"Le nombre d'erreurs que le modèle corrige : \" + str(carracter_corrected)\n",
    "print 'Le nombre de carractère correcte par viterbi :' +str(correct_carracter)\n",
    "print \"L'accuarcy du modèle est :\"+str(correct_carracter*100.0/total_carracter ) + \" %\"\n",
    "print \"L'erreur du modèle est : \"+str(100 - correct_carracter*100.0/total_carracter ) + \" %\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
